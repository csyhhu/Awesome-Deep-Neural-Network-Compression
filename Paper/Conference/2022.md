# ICLR

## Quantization
### Model Quantization (Quantization for Efficient Inference)
- Bibert: Accurate Fully Binarized Bert
- F8NET:  Fixed-point 8-bit Only Multiplication for Network Quantization

#### Post-Training Quantization
- Qdrop: Randomly Dropping Quantization for Extremely Low-bit Post-training Quantization
- SQUANT: On-the-fly Data-free Quantization Via Diagonal Hessian Approximation

### Quantization Training (Quantization for Efficient Training )
- How Low Can We Go: Trading Memory for Error in Low-Precision Training
- Adaptive Gradient Quantization for Data-Parallel SGD
- On Distributed Adaptive Optimization with Gradient Compression
- Toward Efficient Low-precision Training: Data Format Optimization and Hysteresis Quantization

### Theory
- VC Dimension of Partially Quantized Neural Networks in the Overparametrized Regime
- Information Bottleneck: Exact Analysis of (quantized) Neural Networks

## Pruning
### Lottery Ticket
- Dual Lottery Ticket Hypothesis
- Peek-a-boo: What (more) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently
- Proving the Lottery Ticket Hypothesis: Pruning is All You Need
- Plant 'n' Seek: Can You Find the Winning Ticket?
- Signing the Supermask: Keep, Hide, Invert
- Prospect Pruning: Finding Trainable Weights at Initialization Using Meta-gradients

### Structure Pruning
- Training Structured Neural Networks Through Manifold Identification and Variance Reduction
- Effective Model Sparsification by Scheduled Grow-and-prune Methods
- Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions
- An Operator Theoretic Perspective on Pruning Deep Neural Networks
- SOSP: Efficiently Capturing Global Correlations by Second-order Structured Pruning

### Unstructure Pruning
- Encoding Weights of Irregular Sparsity for Fixed-to-fixed Model Compression
- Learning Pruning-friendly Networks Via Frank-wolfe: One-shot, Any-sparsity, and No Retraining

### Sparse Training
- MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge


## Other Compression (Distillation, Low-Rank, Clustering)
- Exploring Extreme Parameter Compression for Pre-trained Language Models
- Language Model Compression with Weighted Low-rank Factorization
- Dkm: Differentiable K-means Clustering Layer for Neural Network Compression
