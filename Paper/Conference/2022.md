# NeurIPS

## Quantization
- Leveraging Inter-Layer Dependency for Post -Training Quantization
- Redistribution of Weights and Activations for AdderNet Quantization
- Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer
- XTC: Extreme Compression for Pre-trained Transformers Made Simple and Efficient
- BiT: Robustly Binarized Multi-distilled Transformer
- Hierarchical Channel-spatial Encoding for Communication-efficient Collaborative Learning

### Quantization Training
- Quantized Training of Gradient Boosting Decision Trees
- On-Device Training Under 256KB Memory
- Is Integer Arithmetic Enough for Deep Learning Training?

### Post-Training Quantization
- Towards Efficient Post-training Quantization of Pre-trained Language Models 
- Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning
- ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers
- Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning

## Pruning
- Data-Efficient Structured Pruning via Submodular Optimization
- A Fast Post-Training Pruning Framework for Transformers
- Back Razor: Memory-Efficient Transfer Learning by Self-Sparsified Backpropagation
- SAViT: Structure-Aware Vision Transformer Pruning
via Collaborative Optimization
- Prune and distill: similar reformatting of image information along rat visual cortex and deep neural networks
- Recall Distortion in Neural Network Pruning and the Undecayed Pruning Algorithm 
- Structural Pruning via Latency-Saliency Knapsack
- Sparse Probabilistic Circuits via Pruning and Growing
- Learning Best Combination for Efficient N:M Sparsity

### Lottery
- Advancing Model Pruning via Bi-level Optimization
- Rare Gems: Finding Lottery Tickets at Initialization
- Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations
- Sparse Winning Tickets are Data-Efficient Image Recognizers
- Analyzing Lottery Ticket Hypothesis from PAC-Bayesian Theory Perspective

## Theory
- ClimbQ: Class Imbalanced Quantization Enabling Robustness on Efficient Inferences
- Pruningâ€™s Effect on Generalization Through the Lens of Training and Regularization
- Pruning has a disparate impact on model accuracy
- Analyzing Lottery Ticket Hypothesis from PAC-Bayesian Theory Perspective


## Compression
- Beyond neural scaling laws: beating power law scaling via data pruning
- VTC-LFC: Vision Transformer Compression with Low-Frequency Components
- SInGE: Sparsity via Integrated Gradients Estimation of Neuron Relevance
- Weighted Mutual Learning with Diversity-Driven Model Compression
- Resource-Adaptive Federated Learning with All-In-One Neural Composition
- EfficientFormer: Vision Transformers at MobileNet Speed

## Efficient Training
- LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning
- The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models
- Polyhistor: Parameter-Efficient Multi-Task Adaptation for Dense Vision Tasks
  
# ICML

## Quantization
### Model Quantization
- SDQ: Stochastic Differentiable Quantization with Mixed Precision
- Fast Lossless Neural Compression with Integer-Only Discrete Flows

### Quantization Training
- Secure Quantized Training for Deep Learning
- DAdaQuant: Doubly-adaptive quantization for communication-efficient
Federated Learning
- Overcoming Oscillations in Quantization-Aware Training
- Optimal Clipping and Magnitude-aware Differentiation
for Improved Quantization-aware Training

## Pruning
- The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks
- PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance
- PAC-Net: A Model Pruning Approach to Inductive Transfer Learning
- Prune and distill: similar reformatting of image information along rat visual cortex and deep neural networks
### Lottery Ticket
- Winning the Lottery Ahead of Time: Efficient Early Network Pruning

### Theory
- Sparse Double Descent: Where Network Pruning Aggravates Overfitting
- Neural Network Pruning Denoises the Features and Makes Local Connectivity Emerge in Visual Tasks
- Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness

## Compression
- DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks
- History Compression via Language Models in Reinforcement Learning
- Spatial-Channel Token Distillation for Vision MLPs
  
### Efficient Training
- GACT: Activation Compressed Training for Generic Network Architectures
- Lightweight Projective Derivative Codes for Compressed Asynchronous Gradient Descent
    
# ICLR

## Quantization
### Model Quantization (Quantization for Efficient Inference)
- Bibert: Accurate Fully Binarized Bert
- F8NET:  Fixed-point 8-bit Only Multiplication for Network Quantization

#### Post-Training Quantization
- Qdrop: Randomly Dropping Quantization for Extremely Low-bit Post-training Quantization
- SQUANT: On-the-fly Data-free Quantization Via Diagonal Hessian Approximation

### Quantization Training (Quantization for Efficient Training )
- How Low Can We Go: Trading Memory for Error in Low-Precision Training
- Adaptive Gradient Quantization for Data-Parallel SGD
- On Distributed Adaptive Optimization with Gradient Compression
- Toward Efficient Low-precision Training: Data Format Optimization and Hysteresis Quantization

### Theory
- VC Dimension of Partially Quantized Neural Networks in the Overparametrized Regime
- Information Bottleneck: Exact Analysis of (Quantized) Neural Networks

## Pruning
### Lottery Ticket
- Dual Lottery Ticket Hypothesis
- Peek-a-boo: What (more) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently
- Proving the Lottery Ticket Hypothesis: Pruning is All You Need
- Plant 'n' Seek: Can You Find the Winning Ticket?
- Signing the Supermask: Keep, Hide, Invert
- Prospect Pruning: Finding Trainable Weights at Initialization Using Meta-gradients

### Structure Pruning
- Training Structured Neural Networks Through Manifold Identification and Variance Reduction
- Effective Model Sparsification by Scheduled Grow-and-prune Methods
- Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions
- An Operator Theoretic Perspective on Pruning Deep Neural Networks
- SOSP: Efficiently Capturing Global Correlations by Second-order Structured Pruning

### Unstructure Pruning
- Encoding Weights of Irregular Sparsity for Fixed-to-fixed Model Compression
- Learning Pruning-friendly Networks Via Frank-wolfe: One-shot, Any-sparsity, and No Retraining

### Sparse Training
- MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge


## Other Compression (Distillation, Low-Rank, Clustering)
- Exploring Extreme Parameter Compression for Pre-trained Language Models
- Language Model Compression with Weighted Low-rank Factorization
- Dkm: Differentiable K-means Clustering Layer for Neural Network Compression
