# ICLR

## Quantization
- OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models
- LoftQ: Lora-Fine-Tuning-Aware Quantization for Large Language Models
- EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models
- QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models
- QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models
- LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models
- LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning
- LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection
- Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models
- PB-LLM: Partially Binarized Large Language Models
- AffineQuant: Affine Transformation Quantization for Large Language Models
- Transformer-VQ: Linear-Time Transformers via Vector Quantization
- Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators

## KV Cache
- Model Tells You What to Discard: Adaptive Kv Cache Compression for LLMs

## Pruning
- Scaling Laws for Sparsely-Connected Foundation Models
- What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity
- Balancing Act: Constraining Disparate Impact in Sparse Models
- Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models
- A Simple and Effective Pruning Approach for Large Language Models
- Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging
- Adaptive Sharpness-Aware Pruning for Robust Sparse Networks
- Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance
- Data-independent Module-aware Pruning for Hierarchical Vision Transformers
- The LLM Surgeon
- BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation
- ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models
- Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models
- SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning
- Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning
- FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity
- The Need for Speed: Pruning Transformers with One Recipe
- NeurRev: Train Better Sparse Neural Network Practically via Neuron Revitalization

## Compression
- Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings
- In defense of parameter sharing for model-compression

## Efficient Training
- ZeRO++: Extremely Efficient Collective Communication for Large Model Training
- DeepZero: Scaling Up Zeroth-Order Optimization for Deep Model Training

## Lottery Ticket
- Graph Lottery Ticket Automated

## Dataset Compression
- $\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning

## Benchmark & Analysis
- Compressing LLMs: The Truth is Rarely Pure and Never Simple
- The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning

# ICML

## Compression
- FLEXTRON: Many-in-One Flexible Large Language Model

## Attention Acceleration
- I/O Complexity of Attention, or How Optimal is FlashAttention?
- Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers

## Quantization
- Data-free Neural Representation Compression with Riemannian Neural Dynamics
- Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs
- Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization
- ERQ: Error Reduction for Post-Training Quantization of Vision Transformers

## Low Rank
- DoRA: Weight-Decomposed Low-Rank Adaptation
- Accurate LoRA-Finetuning Quantization of LLMs via Information Retention

## Pruning
- Sparse is Enough in Fine-tuning Pre-trained Large Language Models