# NeurIPS

## Quantization
- DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs
- KV Cache is 1 Bit Per Channel Efficient Large Language Model Inference with Coupled Quantization
- Exploiting LLM Quantization
- BiDM: Pushing the Limit of Quantization for Diffusion Models
- QBB: Quantization with Binary Bases for LLMs
- StepbaQ: Stepping backward as Correction for Quantized Diffusion Models
- PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression
- Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models

### Post-Training Quantization
- Q-VLM: Post-training Quantization for Large Vision-Language Models
- PTQ4DiT: Post-training Quantization for Diffusion Transformers
- Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers
- MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization 

### Quantization Training
- SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training
- Don't Compress Gradients in Random Reshuffling: Compress Gradient Differences *

## Low Rank
- LoQT: Low Rank Adapters for Quantized Training *
- Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters
- QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation
- Compressing Large Language Models using Low Rank and Low Precision Decomposition

## KV Cache
- KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization *
- ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification
- MiniCache: KV Cache Compression in Depth Dimension for Large Language Models

## Pruning
- Compact Language Models via Pruning and Knowledge Distillation *
- Finding Transformer Circuits With Edge Pruning
- SparseLLM: Towards Global Pruning of Pre-trained Language Models
- DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization
- DEPrune: Depth-wise Separable Convolution Pruning for Maximizing GPU Parallelism
- Pruning neural network models for gene regulatory dynamics using data and domain knowledge
- Fast Iterative Hard Thresholding Methods with Pruning Gradient Computations
- How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective
- Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective
- FM-Delta: Lossless Compression for Storing Massive Fine-tuned Foundation Models

### Structure Pruning
- S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training *
- DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models
- SlimGPT: Layer-wise Structured Pruning for Large Language Models
- SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization

### Sparsity Allocation
- Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models *
- AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models

## Compression
- LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment

# ICLR

## Quantization
- OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models
- EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models
- QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models
- LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models
- LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection
- Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models
- PB-LLM: Partially Binarized Large Language Models
- AffineQuant: Affine Transformation Quantization for Large Language Models
- Transformer-VQ: Linear-Time Transformers via Vector Quantization
- Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators

## Low Rank
- Batched Low-Rank Adaptation of Foundation Models
- QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models
- LoftQ: Lora-Fine-Tuning-Aware Quantization for Large Language Models
- LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning
- LONGLORA: Efficient Fine-Tuning Of Long Context Large Language Models

## KV Cache
- Model Tells You What to Discard: Adaptive Kv Cache Compression for LLMs

## Pruning
- Scaling Laws for Sparsely-Connected Foundation Models
- What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity
- Balancing Act: Constraining Disparate Impact in Sparse Models
- Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models
- A Simple and Effective Pruning Approach for Large Language Models
- Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging
- Adaptive Sharpness-Aware Pruning for Robust Sparse Networks
- Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance
- Data-independent Module-aware Pruning for Hierarchical Vision Transformers
- The LLM Surgeon
- BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation
- ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models
- Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models
- SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning
- Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning
- FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity
- The Need for Speed: Pruning Transformers with One Recipe
- NeurRev: Train Better Sparse Neural Network Practically via Neuron Revitalization

## Compression
- Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings
- In defense of parameter sharing for model-compression

## Efficient Training
- ZeRO++: Extremely Efficient Collective Communication for Large Model Training
- DeepZero: Scaling Up Zeroth-Order Optimization for Deep Model Training

## Lottery Ticket
- Graph Lottery Ticket Automated

## Dataset Compression
- $\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning

## Benchmark & Analysis
- Compressing LLMs: The Truth is Rarely Pure and Never Simple
- The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning

# ICML

## Compression
- FLEXTRON: Many-in-One Flexible Large Language Model

## Attention Acceleration
- I/O Complexity of Attention, or How Optimal is FlashAttention?
- Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers

## Quantization
- Data-free Neural Representation Compression with Riemannian Neural Dynamics
- Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs
- Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization
- ERQ: Error Reduction for Post-Training Quantization of Vision Transformers

## Low Rank
- DoRA: Weight-Decomposed Low-Rank Adaptation
- Accurate LoRA-Finetuning Quantization of LLMs via Information Retention

## Pruning
- Sparse is Enough in Fine-tuning Pre-trained Large Language Models