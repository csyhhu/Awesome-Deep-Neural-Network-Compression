# ICLR

## Quantization
- OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting
- Systematic Outliers in Large Language Models
- Efficient Low-Bit Quantization with Adaptive Scales for Multi-task Co-Training
- ARB-LLM: Alternating Refined Binarizations for Large Language Models
- Quamba: A Post-Training Quantization Recipe for Selective State Space Models

## Pruning
- You Only Prune Once: Designing Calibration-Free Model Compression with Policy Learning
- Adaptive Pruning of Pretrained Transformer via Differential Inclusions
- StbLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs
- Beware of Calibration Data for Pruning Large Language Models

## Distillation
- Improving Language Model Distillation Through Hidden State Matching

## Compression
- MCNC: Manifold-Constrained Reparameterization for Neural Compression
- Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression
- Mixture Compressor for Mixture-of-Experts LLMs Gains More
- Forget the Data and Fine-Tuning! Just Fold the Network to Compress
- BitStack: Any-Size Compression of Large Language Models in Variable Memory Environments
- LlamaFlex: Many-in-One LLMs via Generalized Pruning and Weight Sharing
- ModeGPT: Modular Decomposition for Large Language Model Compression

## Efficient Attention
- Rodimus Breaking the Accuracy Efficiency Trade Off with Efficient Attentions

## Token Compression
- Llava-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token

## Long Context
- Long Context Compression with Activation Beacon
- Melodi: Exploring Memory Compression for Long Contexts
- Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling
- LongPO: Long Context Self-Evolution of Large Language Models Through Short-Tolong Preference Optimization

## KV Cache
- Palu: Kv-Cache Compression with Low-Rank Projection
- MatryoshkaKV: Adaptive KV Compression
- RazorAttention: Efficient Kv Cache Compression Through Retrieval Heads
- Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning
- VL-Cache: Sparsity and Modality-Aware Kv Cache Compression for Vision-Language Model Inference Acceleration
- SqueezeAttention: 2d Management of KV cache in LLM Inference via Layer-Wise Optimal Budget
- ScBench: A Kv Cache-Centric Analysis of Long-Context Methods

## Low Rank
- SVD-LLM: Truncation-Aware Singular Value Decomposition for Large Language Model Compression
- Dobi-SVD: Differentiable Svd for LLM Compression and Some New Perspectives
- On the Optimization Landscape of Low Rank Adaptation Methods for Large Language Models
- Two Sparse Matrices Are Better Than One: Sparsifying Neural Networks with Double Sparse Factorization

## Efficient Training
- COAT: Compressing Optimizer States and Activation for Memory Efficient FP8 Training
- Maintaining Structural Integrity in Parameter Spaces for Parameter Efficient Fine-Tuning