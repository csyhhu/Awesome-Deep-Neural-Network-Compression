# Low-Rank / LORA
- LoRA: Low-Rank Adaptation of Large Language Models
- LoQT: Low Rank Adapters for Quantized Training
- Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters
- QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation
- Compressing Large Language Models using Low Rank and Low Precision Decomposition