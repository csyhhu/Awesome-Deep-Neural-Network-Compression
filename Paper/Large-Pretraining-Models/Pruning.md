# Pruning
- SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot
- Finding Transformer Circuits With Edge Pruning
- SparseLLM: Towards Global Pruning of Pre-trained Language Models
- DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization
- DEPrune: Depth-wise Separable Convolution Pruning for Maximizing GPU Parallelism
- Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective
- FM-Delta: Lossless Compression for Storing Massive Fine-tuned Foundation Models
- LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment

## Structure Pruning
- S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training
- DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models
- SlimGPT: Layer-wise Structured Pruning for Large Language Models

## Sparsity Allocation
- Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models
- AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models