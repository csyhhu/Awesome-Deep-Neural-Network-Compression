![Maintenance](https://img.shields.io/maintenance/yes/2021.svg?color=red&style=flat-square)
![GitHub last commit](https://img.shields.io/github/last-commit/csyhhu/Awesome-Deep-Neural-Network-Compression.svg?style=flat-square)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/csyhhu/Awesome-Deep-Neural-Network-Compression.svg?style=flat-square)
[![Ask Me Anything !](https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg?style=flat-square)](https://GitHub.com/Naereen/ama)
[![Awesome](https://awesome.re/badge.svg?style=flat-square)](https://awesome.re)
# Awesome Deep Neural Network Compression

This is a developing branch focused on implementation of state-of-the-art model compression method.

Please refer to the `main` branch for paper list.

## Under Progress
- [INQ](./Codes/INQ): [Incremental Network Quantization]()
- [QIL](./Codes/QIL): [Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss]()
- [Asymmetric-MAQ-XNOR](./Codes/Asymmetric-MAQ-XNOR): A training-aware quantization method to accumulate layer-wise quantization threshold.
- [Asymmetric-LTH](./Codes/Asymmetric-LTH): A training-aware quantization method to learn layer-wise quantization threshold, inspired by [Learned Step Size Quantization](https://arxiv.org/abs/1902.08153)